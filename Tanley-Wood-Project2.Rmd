---
title: "Tanley-Wood-Project2"
author: "Jordan Tanley and Jonathan Wood"
date: '2022-07-05'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction - Jonathan

## Data

The data in this analysis will be the [online news popularity dataset](https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity). This data has a set of features on articles from Mashable.com over a two year period. 

The goal of this project is to determine the number of shares (how many times the article was shared over social media) the article has. We will use this information to predict if an article can be popular by the number of shares.

## Notable Variables

While there are 61 variables in the data set, we will not use all of them for this project. The notable variables are the following:

- "shares" - the number of shares the article has gotten over social media. This is the label or variable we want our models to predict for new articles
- "data_channel_is" - a set of variables that tells if the article is in a particular category, such as business, sports, or lifestyle.
- "weekday_is" - a set of variables that tells what day of the week the article was published on.
- "num_keywords" - the number of keywords within the article
- "num_images" - the number of images within the article
- "num_videos" - the number of videos within the article

## Methods

Multiple methods will be used for this project to predict the number of shares a new article can generate, including

- Linear regression
- Tree-based models
  - Random forest
  - Boosted tree

# Data - Jordan

```{r, include=FALSE}
library(readr)
library(tidyverse)
library(ggplot2)
library(knitr)
library(gridExtra)
library(caret)
```

In order to read in the data using a relative path, be sure to have the data file saved in your working directory.  

```{r}
# read in the data
news <- read_csv("OnlineNewsPopularity/OnlineNewsPopularity.csv")

head(news)
```

```{r} 
#DONT FORGET TO DROP THIS VARIABLE WHEN DOING RANDOM FOREST AND BOOSTED TREE
news$weekday <- ifelse(news$weekday_is_friday == 1, "Friday",
                       ifelse(news$weekday_is_monday == 1, "Monday",
                              ifelse(news$weekday_is_tuesday == 1, "Tuesday",
                                     ifelse(news$weekday_is_wednesday == 1, "Wednesday",
                                            ifelse(news$weekday_is_thursday == 1, "Thursday",
                                                   ifelse(news$weekday_is_saturday == 1, "Saturday", 
                                                          "Sunday"))))))
```

Next, let's subset the data so that we can only look at the data channel of interest. We will look at articles with the "Social Media" data channel.

```{r}
SocialMedia <- news %>% 
                as_tibble() %>% 
                filter(data_channel_is_socmed == 1) %>% 
                select(-c(url, timedelta, starts_with("data_channel")))
head(SocialMedia)
```
# Summarizations - Both (3 plots each)

```{r}
# Contingency table of frequencies for days of the week
kable(table(SocialMedia$weekday), 
      col.names = c("Weekday", "Frequency"), 
      caption = "Contingency table of frequencies for days of the week")

# Numerical Summary of Shares
SocialMedia %>% summarise(Minimum = min(shares), 
                          Q1 = quantile(shares, prob = 0.25), 
                          Average = mean(shares), 
                          Median = median(shares), 
                          Q3 = quantile(shares, prob = 0.75), 
                          Maximum = max(shares)) %>% 
                kable(caption = "Numerical Summary of Shares")

# Numerical Summary of Number of words in the content
SocialMedia %>% summarise(Minimum = min(n_tokens_content), 
                          Q1 = quantile(n_tokens_content, prob = 0.25), 
                          Average = mean(n_tokens_content), 
                          Median = median(n_tokens_content), 
                          Q3 = quantile(n_tokens_content, prob = 0.75), 
                          Maximum = max(n_tokens_content)) %>% 
                kable(caption = "Numerical Summary of Number of words in the content")

# Numerical Summary of Number of words in the content for the upper quantile of Shares
SocialMedia %>% filter(shares > quantile(shares, prob = 0.75)) %>%
                summarise(Minimum = min(n_tokens_content), 
                          Q1 = quantile(n_tokens_content, prob = 0.25), 
                          Average = mean(n_tokens_content), 
                          Median = median(n_tokens_content), 
                          Q3 = quantile(n_tokens_content, prob = 0.75), 
                          Maximum = max(n_tokens_content)) %>% 
                kable(caption = "Numerical Summary of Number of words in the content for the upper quantile of Shares")

```

```{r}
kable(table(SocialMedia$n_tokens_content),
  col.names = c("Tokens", "Frequency"), 
  caption = "Contingency table of frequencies for number of tokens in the article content")

# Summarizing the number of images in the article
SocialMedia %>% 
  summarise(Minimum = min(num_imgs), 
      Q1 = quantile(num_imgs, prob = 0.25), 
      Average = mean(num_imgs), 
      Median = median(num_imgs), 
      Q3 = quantile(num_imgs, prob = 0.75), 
      Maximum = max(num_imgs)) %>% 
  kable(caption = "Numerical summary of number of images in an article")

# Summarizing the number of videos in the article
SocialMedia %>% 
  summarise(Minimum = min(num_videos), 
      Q1 = quantile(num_videos, prob = 0.25), 
      Average = mean(num_videos), 
      Median = median(num_videos), 
      Q3 = quantile(num_videos, prob = 0.75), 
      Maximum = max(num_videos)) %>% 
  kable(caption = "Numerical summary of number of videos in an article")

# Summarizing the number of positive word rate
SocialMedia %>% 
  summarise(Minimum = min(rate_positive_words), 
      Q1 = quantile(rate_positive_words, prob = 0.25), 
      Average = mean(rate_positive_words), 
      Median = median(rate_positive_words), 
      Q3 = quantile(rate_positive_words, prob = 0.75), 
      Maximum = max(rate_positive_words)) %>% 
  kable(caption = "Numerical Summary of the rate of positive words in an article")

# Summarizing the number of negative word rate
SocialMedia %>% 
  summarise(Minimum = min(rate_negative_words), 
      Q1 = quantile(rate_negative_words, prob = 0.25), 
      Average = mean(rate_negative_words), 
      Median = median(rate_negative_words), 
      Q3 = quantile(rate_negative_words, prob = 0.75), 
      Maximum = max(rate_negative_words)) %>% 
  kable(caption = "Numerical Summary of the rate of negative words in an article")
```

```{r}
# Boxplot of Shares for Each Weekday
ggplot(SocialMedia, aes(x = weekday, y = shares)) + 
          geom_boxplot(fill = "grey") + 
          labs(x = "Weekday", title = "Boxplot of Shares for Each Weekday", y = "Shares") + 
          theme_classic()

# Scatterplot of Number of words in the content vs Shares
ggplot(SocialMedia, aes(x = n_tokens_content, y = shares)) + 
          geom_point(color = "grey") +
          labs(x = "Number of words in the content", y = "Shares", 
               title = "Scatterplot of Number of words in the content vs Shares") +
          theme_classic()

# Scatterplot of Number of words in the title vs Shares
ggplot(SocialMedia, aes(x = n_tokens_title, y = shares)) + 
          geom_point(color = "grey") +
          labs(x = "Number of words in the title", y = "Shares", 
               title = "Scatterplot of Number of words in the title vs Shares") +
          theme_classic()

```

```{r}
ggplot(SocialMedia, aes(x=shares)) +
  geom_histogram(color="grey") +
  labs(x = "Number of images in an article", 
               title = "Histogram of number of shares") +
  theme_classic()

ggplot(SocialMedia, aes(x=rate_positive_words, y=shares)) +
  geom_point(color="grey") +
  labs(x = "Number of images in an article", y = "Shares", 
               title = "Scatterplot of rate of positive words in an article vs shares") +
  theme_classic()

ggplot(SocialMedia, aes(x=rate_negative_words, y=shares)) +
  geom_point(color="grey") +
  labs(x = "Number of images in an article", y = "Shares", 
               title = "Scatterplot of rate of negative words in an article vs shares") +
  theme_classic()

ggplot(SocialMedia, aes(x=global_sentiment_polarity, y=shares)) +
  geom_boxplot(color="grey") +
  labs(x = "Number of images in an article", y = "Shares", 
               title = "Scatterplot of global sentiment polarity in an article vs shares") +
  theme_classic()
```


```{r}
SocialMedia <- subset(SocialMedia, select = -c(weekday))
```


# Modeling

## Splitting the Data

First, let's split up the data into a testing set and a training set using the proportions: 70% training and 30% testing.

```{r}
set.seed(9876)
# Split the data into a training and test set (70/30 split)
# indices
train <- sample(1:nrow(SocialMedia), size = nrow(SocialMedia)*.70)
test <- setdiff(1:nrow(SocialMedia), train)

# training and testing subsets
Training <- SocialMedia[train, ]
Testing <- SocialMedia[test, ]
```

## Linear Models

INSERT EXPLANATION HERE! (note to myself - not yelling at you! lol)

Linear Model #1: - Jordan

```{r, warning=FALSE}
fit1 <- train(shares ~ . , data = Training, method = "lm",
              preProcess = c("center", "scale"), 
              trControl = trainControl(method = "cv", number = 5))
```

Linear Model #2: - Jonathan

```{r}
lm_fit <- train(
  shares ~ .^2,
  data=Training,
  method="lm",
  preProcess = c("center", "scale"), 
  trControl = trainControl(method = "cv", number = 5)
)
```


## Random Forest - Jordan

INSERT EXPLANATION HERE! (note to myself - not yelling at you! lol)

```{r}
ranfor <- train(shares ~ ., data = Training, method = "rf", preProcess = c("center", "scale"),
                trControl = trainControl(method = "cv", number = 5), 
                tuneGrid = expand.grid(mtry = c(1:round(ncol(Training)/3))))

ranfor
```



## Boosted Tree - Jonathan

```{r}
tune_grid <- expand.grid(
  n.trees = c(5, 10, 50, 100),
  interaction.depth = c(1,2,3, 4),
  shrinkage = 0.1,
  n.minobsinnode = 10
)

bt_fit <- train(
  shares ~ .,
  data=Training,
  method="gbm",
  preProcess = c("center", "scale"), 
  trControl = trainControl(method = "cv", number = 5)
)

bt_fit
```


# Comparison - Jordan

```{r, warning=FALSE}
predRF <- predict(ranfor, newdata = Testing)
RF <- postResample(predRF, Testing$shares)

predlm1 <- predict(fit1, newdata = Testing)
LM <- postResample(predlm1, Testing$shares)

# NEEDS TO BE REPEATED FOR OTHER TWO MODELS - I'll do this later!

dat <- data.frame(rbind(t(data.frame(LM)), t(data.frame(RF))))
df <- as_tibble(rownames_to_column(dat, "models"))

best <- df %>% filter(RMSE == min(RMSE)) %>% select(models)

paste("The Best fitting model according to RMSE is", best$models, sep = " ")
```


# Automation - Jonathan

```{r, eval=FALSE}
#rmarkdown::render(
#  "Tanley-Wood-Project2.Rmd",
#  output_format="github_document",
#  output_dir="./Analysis",
#  output_options = list(
#    html_preview = FALSE
#  )
#)
```


